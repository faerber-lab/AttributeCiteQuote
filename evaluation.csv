metric name,metric abbreviation,framework,evaluation method,evaluation dimension,decription,source
Attributable to Identified Sources (paragraph-level),AIS-paragraph,,human evaluation,attribution,"Given a query q, a short answer a and a passage candidate p, we frame attribution detection as a binary classification task: (q, a, p) ‚àí> ais with ais ‚àà {0, 1}. 1 corresponds to the attributed class (i.e., the answer is attributed to the passage) and 0 corresponds to the non-attributed class.",https://aclanthology.org/2023.cl-4.2/
Attributable to Identified Sources (sentence-level),AIS-sentence,,human evaluation,attribution,"Human annotation of AIS score for each sentence of the text passage, if text passage is attributable to a set of evidence, and then report the average AIS score across all sentences",https://aclanthology.org/2023.acl-long.910/
Automatic Attributable to Identified Sources (sentence-level),Auto-AIS-sentence,,inference-based,attribution,"AIS-auto (original version) approximates human AIS judgments. For each sentence s of y, and for each evidence snippet e in A, let NLI(e, s) be the model probability of e entailing s.",https://aclanthology.org/2023.acl-long.910/
Automatic Attributable to Identified Sources (paragraph),Auto-AIS-paragraph,,inference-based,attribution,The metric evaluates to which extent a generated response is attributable to the retrieved documents fed to the LLM for response generation,https://dl.acm.org/doi/10.1145/3627673.3679172
Preservation-Intent,Pres-Intent,,human evaluation,preservation,"Preservation: how much the revised text y preserves aspects of the original text x. We define the binary metric Pres-intent(x, y) to be 1.0 if the revision completely preserves the original intent, and 0.0 otherwise.",https://aclanthology.org/2023.acl-long.910/
Preservation-Levenshtein,Pres-Lev,,lexical overlap,preservation,Preservation: how much the revised text y preserves aspects of the original text x. Simple metric that generally penalizes unnecessary changes based on the character-level Levenshtein edit distance.,https://aclanthology.org/2023.acl-long.910/
Preservation-Combined,Pres-Comb,,"human evaluation, lexical overlap",preservation,"Preservation: how much the revised text y preserves aspects of the original text x. Revision should preserve the original intent while avoiding superfluous edits. To reflect this, Pres-intent and Pres-Lev are combined to one score",https://aclanthology.org/2023.acl-long.910/
F1-attribution-preservation,F1-AP,,"human evaluation, lexical overlap","attribution, preservation",Harmonic mean of AIS-sentence and Pres-comb.,https://aclanthology.org/2023.acl-long.910/
Attribution Accuracy,Attr-Acc,,inference-based,attribution,Accuracy for classification results of attribution detection models.,https://aclanthology.org/2023.emnlp-main.10/
Area under the ROC curve,ROC-AUC,,"inference-based, llm-as-a-judge, lexical overlap",attribution,ROC-AUC for classification results of attribution detection models.,https://icml.cc/2011/papers/385_icmlpaper.pdf
Factual precision in Atomicity Score,FActScore,,"human evaluation, inference-based",attribution,"FActScore involves (1) breaking a generation into a series of atomic facts‚Äîshort statements that each contain one piece of information‚Äî, and (2) assigning a binary label to each atomic fact, allowing a fine-grained evaluation of factual precision. The score can either be calculated via human evaluation or estimated via an LLM/NLI model.",https://aclanthology.org/2023.emnlp-main.741
Recall-Oriented Understudy for Gisting Evaluation ,ROUGE-L,"BEGIN, CitaLaw, ALiiCE, ScholarQABench, CiteKit",lexical overlap,attribution,This category includes n-gram-based metrics that compare the lexical similarity between the response and the knowledge. ,https://aclanthology.org/W04-1013/
Recall-Oriented Understudy for Gisting Evaluation N-Gram,ROUGE-N,,lexical overlap,attribution,The metrics measure the n-gram overlap between a generated text and a gold standard.,https://aclanthology.org/W04-1013/
F1-ngram,F1-ngram,BEGIN,lexical overlap,attribution,This category includes n-gram-based metrics that compare the lexical similarity between the response and the knowledge. Measures the word-level lexical overlap between response and knowledge. ,https://aclanthology.org/2022.tacl-1.62/
BERT-Score,BERT-Score,"BEGIN, CitaLaw",semantic similarity-based,attribution,BERT-Score computes the similarity of the repsonse and the knowledge based on cosine similarity of the sentence embeddings,https://openreview.net/forum?id=SkeHuCVFDr
SciBERT-Score,SciBERT-Score,,semantic similarity-based,attribution,SciBERT-Score computes the similarity of the repsonse and the knowledge based on cosine similarity of the sentence embeddings,https://aclanthology.org/D19-1371/
BART-Score,BART-Score,BEGIN,semantic similarity-based,attribution,The BARTScore for factuality evaluates how accurately a generated text reflects reference content by using a pre-trained BART model to score the likelihood of the reference given the generation.,https://dl.acm.org/doi/10.5555/3540261.3542349
BLEURT,BLEURT,BEGIN,semantic similarity-based,attribution,BLEURT predicts human-like judgements for factuality.,https://aclanthology.org/2020.acl-main.704/
Q2,Q2,BEGIN,question-based,attribution,"Q2 evaluates factuality by generating questions from a candidate response, retrieving supporting answer spans from a knowledge source, and scoring the alignment using question answering and NLI-inspired similarity measures.",https://aclanthology.org/2021.emnlp-main.619/
Citation Precision NLI,Citation Precision NLI,"BEGIN, ALCE, ScholarQABench, CiteKit",inference-based,attribution,"Using NLI-based models to classify the responses in either attributable, unattributable or generic. Citation Precision NLI calculates the classification precision.","https://aclanthology.org/2023.emnlp-main.398, https://aclanthology.org/2022.tacl-1.62/"
Citation Recall NLI,Citation Recall NLI,"BEGIN, ALCE, ScholarQABench, CiteKit",inference-based,attribution,"Using NLI-based models to classify the responses in either attributable, unattributable or generic. Citation Recall NLI calculates the classification recall.","https://aclanthology.org/2023.emnlp-main.398, https://aclanthology.org/2022.tacl-1.62/"
Citation F1 NLI,Citation F1 NLI,"BEGIN, ScholarQABench",inference-based,attribution,"Using NLI-based models to classify the responses in either attributable, unattributable or generic. Citation F1 NLI calculates the harmonic mean between Citation Precision NLI and Citation Recall NLI.",https://aclanthology.org/2022.tacl-1.62/
F1-AttrScore,F1-AttrScore,,inference-based,attribution,"F1-Score for classification of response into attributable, contradictory and extrapolatory",https://aclanthology.org/2023.findings-emnlp.307/
No-Attribution,No-Att,,inference-based,attribution,No-Att shows the percentage of generated sentences lacking any attribution.,https://aclanthology.org/2024.acl-long.182/
Fluency-5,Fluency-5,,human evaluation,linguistic,5-class human evaluation of fluency. 0 (lowest fluency) - 5 (highest fluency),"https://aclanthology.org/2024.acl-long.182/, https://aclanthology.org/2025.coling-main.73/"
Helpfulness-5,Helpfulness-5,,human evaluation,relevance,5-class human evaluation of helpfulness. 0 (lowest helpfulness) - 5 (highest helpfulness),https://aclanthology.org/2024.acl-long.182/
Summary Consistency,SummaC,,inference-based,attribution,SummaC enables NLI models to be successfully used for inconsistency detection in summarization by segmenting documents into sentence units and aggregating scores between pairs of sentences.,https://aclanthology.org/2022.tacl-1.10/
Refusal Recall,Refusal Recall,,lexical overlap,correctness,Refusal Recall quantifies the ability of LLMs to appropriately decline unanswerable questions,https://aclanthology.org/2024.emnlp-demo.10/
MAUVE,MAUVE,"ALCE, CitaLaw, ALiiCE, CiteKit",semantic similarity-based,linguistic,MAUVE is used to evaluate whether the model‚Äôs generated text is fluent and coherent.,https://openreview.net/forum?id=Tqx7nJp7PR
Exact Match Recall,EM Recall,"ALCE, ALiiCE, CiteKit",lexical overlap,correctness,the recall of correct short answers by checking whether the short answers (provided by the dataset) are exact substrings of the generation,https://aclanthology.org/2023.emnlp-main.398/
Exact Match Precision,EM Precision,ALCE,lexical overlap,correctness,"precision of the model prediction, by checking the exact match to the gold answer list",https://aclanthology.org/2023.emnlp-main.398/
Exact Match F1,EM F1,,lexical overlap,correctness,EM F1 score is the harmonic mean of EM precision and EM recall,https://aclanthology.org/2024.emnlp-main.463/
Exact Match,EM,ScholarQABench,lexical overlap,correctness,EM (Accuracy) measures the percentage of predictions that match any one of the ground truth answers exactly.,https://aclanthology.org/D16-1264/
Claim Recall,Claim Recall,ALCE,inference-based,correctness,Usage of the TRUE NLI model to check whether the model output entails ground truth/gold claims,https://aclanthology.org/2023.emnlp-main.398/
Area under the Precision-Recall Curve,AUC-PR,,inference-based,correctness,Classify response into factual and nonfactual. Evaluate with AUC-PR score.,https://aclanthology.org/2024.acl-long.79/
Correctness-Accuracy,Corr-Acc,,inference-based,correctness,Classify response into factual and nonfactual. Evaluate (balanced) accuracy of classification results,https://aclanthology.org/2024.acl-long.79/
Mean Reciprocal Rank,MRR,QUILL,retrieval-based,retrieval,Mean Reciprocal Rank (MRR) is a metric that measures the effectiveness of a ranking system by focusing on the position of the first relevant result in the ranked list returned by the model. It provides insight into how efficiently a system retrieves the top relevant item for a query.,https://aclanthology.org/L02-1301/
Recall@k,Recall@k,,retrieval-based,"retrieval, attribution",How many of the correct items were found in the top k results ,"https://aclanthology.org/2024.acl-long.79/, https://aclanthology.org/2024.emnlp-main.985/"
Precision@k,Precision@k,,retrieval-based,"retrieval, attribution",Precision of retrieved evidence that can be attributed to a generated text (post-generation),https://aclanthology.org/2024.emnlp-main.985/
F1@k,F1@k,,retrieval-based,"retrieval, attribution",Harmonic mean of Recall@k and Precision@k,https://aclanthology.org/2024.emnlp-main.985/
Claim Precision,Claim Precision,,inference-based,correctness,Extract all sub-claims from the system response and calculate the fraction of them being entailed by the reference document,https://aclanthology.org/2024.acl-long.806/
Claim F1,Claim F1,,inference-based,correctness,Harmonic mean of Claim Recall and Claim Precision,https://aclanthology.org/2024.acl-long.806/
Citation Precision Retrieval,Citation Precision Retrieval,RAGE,retrieval-based,citation,"This metric measures if the sentence is correctly supported by each of its citations,  given a gold standard.","https://aclanthology.org/2024.acl-long.806/, https://aclanthology.org/2023.findings-emnlp.467/"
Citation Recall Retrieval,Citation Recall Retrieval,RAGE,retrieval-based,citation,"This metric measures if the sentence comprehensively cites all supporting sources, given a gold standard.","https://aclanthology.org/2024.acl-long.806/, https://aclanthology.org/2023.findings-emnlp.467/"
Citation F1 Retrieval,Citation F1 Retrieval,RAGE,retrieval-based,citation,Harmonic mean of Citation Recall Retrieval and Citation Precison Retrieval,"https://aclanthology.org/2024.acl-long.806/, https://aclanthology.org/2023.findings-emnlp.467/"
Citation Accuracy Retrieval,Citation Accuracy Retrieval,,retrieval-based,citation,Percentage of predicted attributions compared directly against the gold citations,https://aclanthology.org/2024.naacl-long.216/
Attributable to contextual sources,ACS,,human evaluation,attribution,A variant of the AIS score that uses oracle citations from the evaluator rather than the model-generated citations for evaluation.,https://aclanthology.org/2024.acl-long.806/
Hits@k,Hits@k,QUILL,retrieval-based,correctness,"Each question receives a score of 1 if the target answer is present within the predicted LLM answer, and 0 otherwise.",https://aclanthology.org/2024.acl-long.764/
Answer Span,Answer Span,,human evaluation,preservation,Answer Span measures what proportion of the quote is used verbatim by the LLM Answer Composer,https://aclanthology.org/2024.acl-long.764/
Self-Containment,Self-Containment,,human evaluation,attribution,Self-Containment measures whether the answer in the quote is complete or truncated,https://aclanthology.org/2024.acl-long.764/
Pertinence,Pertinence,,human evaluation,retrieval,Pertinence measures how relevant the quote is to the query and whether it is able to answer the query (annotated on a scale of 0 ‚àí 3).,https://aclanthology.org/2024.acl-long.764/
Grounding Sources,Grounding Sources,LLM-Rubric,llm-as-a-judge,attribution,"If the references are provided, to what degree user‚Äôs questions can be answered or resolved using the references? Score between 1 (best) and 4 (worst) are predicted using an LLM. The approach is evaluated using Root Mean Squared Error (RMSE) to compare LLM scores with score from human evaluation.",https://aclanthology.org/2024.acl-long.745/
Citation Presence,Citation Presence,LLM-Rubric,llm-as-a-judge,citation,"Independent of what sources are cited in the conversation, to what degree the claims made by the assistant are followed by a citation. Score between 1 (best) and 4 (worst) are predicted using an LLM. The approach is evaluated using Root Mean Squared Error (RMSE) to compare LLM scores with score from human evaluation.",https://aclanthology.org/2024.acl-long.745/
Citation Suitability,Citation Suitability,LLM-Rubric,llm-as-a-judge,citation,What percentage of citations accurately support the claims made in the conversation? Score between 1 (best) and 4 (worst) are predicted using an LLM. The approach is evaluated using Root Mean Squared Error (RMSE) to compare LLM scores with score from human evaluation.,https://aclanthology.org/2024.acl-long.745/
Citation Optimality,Citation Optimality,LLM-Rubric,llm-as-a-judge,citation,To what degree the cited sources are the best candidates among all the provided sources? Score between 1 (best) and 4 (worst) are predicted using an LLM. The approach is evaluated using Root Mean Squared Error (RMSE) to compare LLM scores with score from human evaluation.,https://aclanthology.org/2024.acl-long.745/
Answer Accuracy@k,Acc_K,,lexical overlap,correctness,"This metric measures the ability to produce a diverse set of correct answers, with a focus on generating at least K of the gold answers.",https://aclanthology.org/2024.emnlp-main.956/
Citation Accuracy,A_C,,lexical overlap,citation,This metric assesses the ability to accurately generate citation strings corresponding to the correct sources.,https://aclanthology.org/2024.emnlp-main.956/
Coverage Score,Coverage Score,,llm-as-a-judge,correctness,Measures the overlap between the generated summary and the insights in the reference documents,https://aclanthology.org/2024.emnlp-main.552/
Citation Score,Citation Score,,lexical overlap,citation,Measures the precision and recall between the generated and gold-standard citations.,https://aclanthology.org/2024.emnlp-main.552/
Joint Score,Joint Score,,"lexical overlap, llm-as-a-judge","correctness, citation","This pieces the Coverage and Citation Scores together, measuring whether a candidate summary both covers the expected insights and cites documents appropriately",https://aclanthology.org/2024.emnlp-main.552/
Classification Macro F1,CF1,,retrieval-based,correctness,F1 score of NLI classification models for evidence classification,https://aclanthology.org/2024.emnlp-main.463/
Evidence F1,Evidence F1,,retrieval-based,attribution,F1 score comparing the predicted evidence with annotated ground truth evidence,https://aclanthology.org/2024.emnlp-main.463/
Unanswerable F1,UF1,,retrieval-based,correctness,F1 score regarding answers that are not answerable with the provided evidence,https://aclanthology.org/2024.emnlp-main.463/
Answer Attribution Agreement,AAA,,inference-based,attribution,Agreement (in %) between human attribution scores using AIS and an ML attribution approach on the same dataset,https://aclanthology.org/2024.emnlp-main.347/
Comprehensiveness 5class,Comp-5,,human evaluation,,"Annotators are asked to rate both comprehensiveness using a 5-point Likert scale, capturing different levels of content coverage.",https://aclanthology.org/2024.emnlp-main.223/
Correctness 5class,Corr-5,,human evaluation,correctness,"Annotators are asked to rate both correctness using a 5-point Likert scale, capturing different levels of content factuality.",https://aclanthology.org/2024.emnlp-main.223/
Disambig-FActScore,D-FActScore,,inference-based,attribution,"D-FActScore is an improved version of FActScore, which better handles entity ambiguity. Unlike FActScore, which checks each fact independently, D-FActScore groups related facts that could be interpreted as referring to the same entity by a reader without prior knowledge. D-FActScore uses entity linking to find the best-matching entity in a knowledge base for each group and verifies all facts in the group using that entity.",https://aclanthology.org/2024.findings-acl.160/
G-Eval Coherence,G-Eval Coherence,G-Eval,llm-as-a-judge,linguistic,Whether the generated text is well-structured and well-organized,https://aclanthology.org/2023.emnlp-main.153/
G-Eval Consistency,G-Eval Consistency,G-Eval,llm-as-a-judge,attribution,Whether the generated text is consistent with the knowledge provided,https://aclanthology.org/2023.emnlp-main.153/
G-Eval Fluency,G-Eval Fluency,G-Eval,llm-as-a-judge,linguistic,Whether the generated text is well-written and grammatical,https://aclanthology.org/2023.emnlp-main.153/
G-Eval Relevance,G-Eval Relevance,G-Eval,llm-as-a-judge,relevance,How well is the generated text relevant to the question,https://aclanthology.org/2023.emnlp-main.153/
Citation Correctness,Citation Correctness,,retrieval-based,citation,"If the generated citation is complete with all three parts, and exactly matches a triplet from the question‚Äôs retrieved KG, correctness = 1.",https://aclanthology.org/2024.findings-acl.28/
Fluency-2,Fluency-2,,human evaluation,linguistic,"The metric evaluates the text fluency using human evaluation (1: fluent, 0: not fluent)",https://aclanthology.org/2024.findings-naacl.277/
Informativeness-2,Informativeness-2,,human evaluation,correctness,"The metric evaluates the extent to which a generated answer answers the question. For each segment, if it responds at least partially to the query, we consider it informative and assign a score of 1; otherwise, it is given a score of 0.",https://aclanthology.org/2024.findings-naacl.277/
Supportedness-2,Supportedness-2,,human evaluation,attribution,"This metric measures whether factual claims in the generated segment can be supported by corresponding quotes. If a cited quote supports the information appearing in the segment it is considered correct. The final score, between 0 and 1, is calculated by dividing the number of correctly cited passages by the total number of passages cited in the segment. If the segment does not cite any passage, it is scored as 0.",https://aclanthology.org/2024.findings-naacl.277/
citation density,citation density,,lexical overlap,citation,the average number of citations in each output sentence,https://aclanthology.org/2024.findings-acl.920/
Disambig-F1,Disambig-F1,,lexical overlap,correctness,Evaluates the fraction of disambiguated questions that can be answered from the predicted long answers,https://aclanthology.org/2022.emnlp-main.566
Disambiguation-Rouge Score/ Combined Score,DR Score,,lexical overlap,"correctness, attribution",Combine ROUGE-L and disambiguation score (Disambig-F1),https://aclanthology.org/2022.emnlp-main.566 
Accuracy NLI,Accuracy NLI,,inference-based,correctness,"The NLI model, evaluates hallucination by predicting whether two texts are factually consistent.","https://huggingface.co/vectara/hallucination_evaluation_model, https://aclanthology.org/2024.naacl-long.167"
DOC-F1,DOC-F1,,retrieval-based,attribution,Check whether the passage(s) cited by the model match the ones cited in the gold reference.,https://aclanthology.org/2024.findings-emnlp.13.pdf
Correct Span Citation Attribution,CSCA,,lexical overlap,attribution,CSCA indicates whether a predicted span is a direct span from the attributed passage.,https://aclanthology.org/2024.findings-emnlp.13.pdf
Distinct Citations,Distinct Citations,RAGE,retrieval-based,citation,Distinct Citations counts the unique citations within the response.,https://aclanthology.org/2024.konvens-main.6/
factual consistency checking model,FactCC,,inference-based,attribution,FactCC is a BERT-based model to verify whether a generated text is faithful to a source text.,https://aclanthology.org/2020.emnlp-main.750/
AlignScore,AlignScore,,inference-based,attribution,AlignScore is a holistic metric to evaluate the factual consistency of generated text to input information.,https://aclanthology.org/2023.acl-long.634/
Discrete LLM-scoring,Discrete LLM-scoring,,llm-as-a-judge,attribution,"Discrete scoring prompts the LLM to assign discrete scores from the set 0, 1, 2 for a given statement and its citation, where
0, 1, and 2 indicate no support, partial support, and full support, respectively",https://aclanthology.org/2024.inlg-main.35/
Continuous LLM-scoring,Continuous LLM-scoring,,llm-as-a-judge,attribution,"Continuous scoring prompts the LLM to assign continuous scores in the range [0, 1] for a given statement and its citation. Here, 1 indicates full support, 0 indicates no support, and values between 0 and 1 indicate partial support.",https://aclanthology.org/2024.inlg-main.35/
Title Accuracy,Title Accuracy,,retrieval-based,citation,Title Accuracy is the percentage of citations recommended by the model that had real paper titles,https://aclanthology.org/2024.hcinlp-1.3/
Author Precision,Author Precision,,retrieval-based,citation,The percentage of authors listed in the model-generated citation who actually appear in the real paper.,https://aclanthology.org/2024.hcinlp-1.3/
Author Recall,Author Recall,,retrieval-based,citation,The percentage of real paper authors that the model correctly included in its generated citation.,https://aclanthology.org/2024.hcinlp-1.3/
Year Accuracy,Year Accuracy,,retrieval-based,citation,"Year Accuracy was calculated by taking the absolute value of the year a real paper was published, minus the year in the model-recommended citation.",https://aclanthology.org/2024.hcinlp-1.3/
Title Relevance,Title Relevance,,retrieval-based,citation,Percent of model-cited papers that are actually cited in the target paper.,https://aclanthology.org/2024.hcinlp-1.3/
Real Author Relevance,Real Author Relevance,,retrieval-based,citation,Percent of cited authors who truly wrote the paper and are also cited in the target paper.,https://aclanthology.org/2024.hcinlp-1.3/
False Author Relevance,False Author Relevance,,retrieval-based,citation,"Percent of cited authors who didn‚Äôt write the paper, but are cited elsewhere in the target paper.",https://aclanthology.org/2024.hcinlp-1.3/
F1 Citation Structure,F1 Citation Structure,,retrieval-based,citation,Structure metrics based on whether the citation trends in the paragraph are similar to the citation trends in the gold standard section. The F1 measures the citation similarity among most similar paragraphs of the gold standard and generated text.,https://aclanthology.org/2024.sdp-1.5/
Adjusted Rand Index,ARI,,structure-based,citation,ARI is a clustering metric to evaluate citation trends of low-similarity paragraphs. Paragraphs in original and generated sections are treated as clusters. ARI evaluates how similar these clusters are in terms of citation distribution.,https://psycnet.apa.org/doiLanding?doi=10.1037%2F1082-989X.9.3.386
Adjusted Rand Index',ARI',,structure-based,citation,ARI' is a modification of ARI to handle unmatched citations that arise due to the multi-paragraph use of citations.,https://aclanthology.org/2024.sdp-1.5/
Mean Absolute Error in Paragraphs,MAEp,,structure-based,citation,Structure metrics based on whether the citation trends in the paragraph are similar to the citation trends in the gold standard section. Measures the average absolute difference in the number of paragraphs between the reference and generated sections.,https://aclanthology.org/2023.emnlp-main.153/
G-Eval-Novelty,G-Eval-Novelty,,semantic similarity-based,novelty,G-Eval-Novelty evaluates whether generated text contains a novelty statement by leveraging prompt-based large language models (LLMs). It combines chain-of-thought (CoT) prompting with a form-filling paradigm to guide the LLM in producing structured evaluation outputs. G-Eval consists of two steps: evaluation step generation and score prediction.,https://aclanthology.org/2023.emnlp-main.153/
Coverage-SBERT,Coverage-SBERT,,semantic similarity-based,attribution,"Coverage: This is an unsupervised metric that intuitively captures the ‚Äúcoverage‚Äù of the content
of a superset in a subset. For this task, we define Coverage (paragraph / sentence levels) to be the average cosine similarity of a slide (bullet point) from the presentation and a
paragraph (sentence) from the document based on their sentence embeddings (Reimers and Gurevych, 2019). Clearly, more is the Coverage, better is the quality.",https://aclanthology.org/2024.findings-emnlp.936/
Keyword Recall,KR,,lexical overlap,citation,"Summarize in 1-2 sentences: KR is a metric that assesses the presence of provided keywords in the generated citation sentence. A higher value signifies that the generated sentence includes the given keywords, indicating good control over keyword incorporation. Given a generated citation sentence S, the keyword recall KR(S) is calculated using ROUGE-L recall to compare the keyword attribute (or a concatenated string of multiple keywords, if applicable) with S.",https://aclanthology.org/2024.sdp-1.4/
Intent Alignment Score,IAS,,llm-as-a-judge,citation,"IAS evaluates the alignment between the generated citation sentence and the specified citation intent. Suppose the control intent is i (one of three possible intents: ‚Äòbackground‚Äô, ‚Äòmethod‚Äô, and ‚Äòresult‚Äô) and LM generates a citation sentence.",https://aclanthology.org/2024.sdp-1.4/
Fluency Score,FS,,llm-as-a-judge,linguistic,FS evaluates how naturally and fluently a generated sentence incorporates keywords by normalizing a SLOR-based language model score using a sigmoid function with an offset to enhance score differentiation.,
METEOR,METEOR,,lexical overlap,attribution,METEOR is an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations.,https://aclanthology.org/W05-0909/
Citation Rate,Citation Rate,,inference-based,attribution,"To rectify the influence of sentence length, Citation Rate is the weighted average of each sentence‚Äôs Citation Recall, with the weights being the number of words in the sentences",https://aclanthology.org/2025.coling-main.349/
Focus Score,Focus Score,,llm-as-a-judge,correctness,The Focus Score examines whether a generated article remains on topic and maintains a clear focus.,https://aclanthology.org/2024.naacl-long.347/
Coverage Score,Coverage Score,,llm-as-a-judge,correctness,The Coverage Score determines if a generated article provides an in-depth exploration of a topic,https://aclanthology.org/2024.naacl-long.347/
Outline Score,Outline Score,,llm-as-a-judge,linguistic,Outline Score checks section heading quality of generated text,https://aclanthology.org/2025.coling-main.349/
Organization Score,Organization Score,,llm-as-a-judge,linguistic,Organization Score evaluates structure and logical connections of generated text,https://aclanthology.org/2025.coling-main.349/
QAFactEval,QAFactEval,,question-based,correctness,"QAFactEval quantifies factual consistency by generating questions from summary statements, answering them using the source document, and measuring the semantic overlap between the generated answers and the original summary content.",https://aclanthology.org/2022.naacl-main.187/
Sem-Rec,Sem-Rec,,lexical overlap,"correctness, attribution",Recall of correct answers/aspects attributed to correct sources.,
Sem-F1,Sem-F1,,lexical overlap,attribution,"Token-F1 of spans attributed to a source vs. gold spans, per source.",https://aclanthology.org/2024.naacl-long.74/
SEMQA Score,SEMQA Score,,"lexical overlap, retrieval-based","attribution, linguistic",Combine ROUGE-L and Sem-F1 score,https://aclanthology.org/2024.naacl-long.74/
Passage-grounded Correctness ,Passage-grounded Correctness,,lexical overlap,correctness,Measures whether atomic units of information from gold answers appear in the generated answers and whether this information is supported by the retrieved passages. Eliminates to score responses that are correct but not grounded in retrieved passages.,https://aclanthology.org/2024.acl-long.641/
Accuracy QA,Accuracy QA,,lexical overlap,correctness,Accuracy measuring question answering performance.,https://aclanthology.org/2025.coling-main.157/
Performance Drop Rate,PDR,,inference-based,preservation,PDR measures the reduction in a model‚Äôs performance when it is presented with perturbed questions relative to the performance on the original questions (lower is better).,https://aclanthology.org/2024.acl-long.163/
Accurately Solved Pairs,ASP,,inference-based,correctness,ASP serves as a metric to evaluate the percentage of cases in which the model successfully addresses both the original question and its perturbed version (higher is better).,https://aclanthology.org/2024.acl-long.163/
Organization-Coherence-5,Organization-Coherence-5,,human evaluation,linguistic,5-class human evaluation of organization and coherence. 0 (lowest) - 5 (highest),https://aclanthology.org/2025.coling-main.73/
Relevance-Target-Paper-5,Relevance-Target-Paper-5,,human evaluation,relevance,5-class human evaluation of relevance to the target paper. 0 (lowest) - 5 (highest),https://aclanthology.org/2025.coling-main.73/
Relevance-Cited-Papers-5,Relevance-Cited-Papers-5,,human evaluation,relevance,5-class human evaluation of relevance to the cited papers. 0 (lowest) - 5 (highest),https://aclanthology.org/2025.coling-main.73/
Facuality-5,Facuality-5,,human evaluation,correctness,5-class human evaluation of factuality. 0 (lowest) - 5 (highest),https://aclanthology.org/2025.coling-main.73/
Usefulness-Informativeness-5,Usefulness-Informativeness-5,,human evaluation,relevance,5-class human evaluation of usefulness and informativeness. 0 (lowest) - 5 (highest),https://aclanthology.org/2025.coling-main.73/
Writing-Style-5,Writing-Style-5,,human evaluation,linguistic,5-class human evaluation of the writing style. 0 (lowest) - 5 (highest),https://aclanthology.org/2025.coling-main.73/
Overall-Quality-5,Overall-Quality-5,,human evaluation,"linguistic, relevance, correctness","5-class human evaluation to get a overall score. Summarizing Organization-Coherence-5, Fluency-5, Relevance-Target-Paper-5, Relevance-Cited-Paper-5, Factuality-5, Usefulness-Informativeness-5 and Writing-Style-5. 0 (lowest) - 5 (highest)",https://aclanthology.org/2025.coling-main.73/
Rep-n,Rep-n,,lexical overlap,linguistic,Reflects the repetition at different n-gram levels in the generated text,https://arxiv.org/abs/1908.04319
Diversity,Diversity,,lexical overlap,linguistic,Evaluates the variety of generated content,https://arxiv.org/abs/1908.04319
Perplexity,Perplexity,,inference-based,linguistic,Perplexity is a measure of the uncertainty or difficulty in predicting the next word in a sequence. A lower perplexity score indicates that the model is more certain about its predictions.,https://aclanthology.org/2021.naacl-main.337/
"Answer-Correctness-3,",Answer-Correctness-3,,human evaluation,correctness,"Was the answer provided correct? Specifically, does the answer provided by ChatGPT align with the ground truth answer? Options: ‚ÄúYes, fully‚Äù, ‚ÄúOnly partially‚Äù, ‚ÄúNo (or no to a large extent)‚Äù.",https://dl.acm.org/doi/10.1145/3624918.3625329
References-Provision-2,References-Provision-2,,human evaluation,citation,Are references provided in the answer? This was a binary ques- tion (yes/no).,https://dl.acm.org/doi/10.1145/3624918.3625329
Reference-Existence-2,Reference-Existence-2,,human evaluation,citation,"For each reference, does it actually exist? For a journal refer- ence, does the paper with that title exist? Does the suggested Wikipedia article exist? Options: ‚ÄúYes‚Äù, ‚ÄúNo‚Äù",https://dl.acm.org/doi/10.1145/3624918.3625329
URL-Provision-2,URL-Provision-2,,human evaluation,citation,"Is a URL provided with the references that actually points to mentioned source? Options: ‚ÄúYes‚Äù, ‚ÄúNo‚Äù.",https://dl.acm.org/doi/10.1145/3624918.3625329
Attribution-3,Attribution-3,,human evaluation,attribution,"Does the statement about the reference in ChatGPT‚Äôs answer align with the actual reference content? Options: ‚ÄúYes, fully‚Äù, ‚ÄúOnly partially‚Äù, ‚ÄúNo (or no to a large extent)‚Äù.",https://dl.acm.org/doi/10.1145/3624918.3625329
Paragraph Count,Paragraph Count,,structure-based,linguistic,Count of the generated paragraphs to check if the LLM follows the instructions,https://aclanthology.org/2024.acl-long.265/
Citation Mark Percentage,Citation Mark Percentage,,structure-based,citation,Percentage of utilization of citation mark in generated citation texts to check if the LLM follows the instructions,https://aclanthology.org/2024.acl-long.265/
N-Gram Overlap,N-Gram Overlap,,lexical overlap,preservation,The n-gram overlap between the input and the model output to check whether the model copies from the prompt.,https://aclanthology.org/2024.acl-long.265/
Query Correctness Rate,Query Correctness Rate,,human evaluation,correctness,Percentage of the RAG queries that retrieved the correct document and returned a correct answer,https://dl.acm.org/doi/10.1145/3688671.3688784
Citation Overlap Precision,Citation Overlap Precision,,retrieval-based,attribution,evaluation of citation attributability by computing for each query the overlap between the gold citations Cùëîùëúùëôùëë and the whole citations in the generated answer Cùëîùëíùëõ. ,https://dl.acm.org/doi/10.1145/3627673.3679172
Citation Overlap Recall,Citation Overlap Recall,,retrieval-based,attribution,evaluation of citation attributability by computing for each query the overlap between the gold citations Cùëîùëúùëôùëë and the whole citations in the generated answer Cùëîùëíùëõ. ,https://dl.acm.org/doi/10.1145/3627673.3679172
Answer Correctness F1,F1AC,Trust-Score,retrieval-based,correctness,F1AC serves as a comprehensive measure of how well the LLM grounds its claims on the document.,https://iclr.cc//virtual/2025/poster/30139
Grounded Refusals F1,F1GR,Trust-Score,inference-based,correctness,This metric evaluates the model‚Äôs refusal performance (the LLMs ability to identify when a response is unanswerable based on the provided documents) by calcuating dataset-wide precision and recall for both ground-truth answerable cases and refusals.,https://iclr.cc//virtual/2025/poster/30139
Grounded Citations F1,F1GC,Trust-Score,inference-based,attribution,F1GC quantifies the Groundedness of Citations.,https://iclr.cc//virtual/2025/poster/30139
Answered Ratio in %,AR%,,retrieval-based,attribution,"This metric measures the answering tendency of an LLM, or the responsiveness. A model is expected to show a high AR% for answerable questions and
a low AR% for unanswerable ones, with the scores expected to align with the dataset distribution.",https://iclr.cc//virtual/2025/poster/30139
Relaxed Exact Match,Relaxed EM,,lexical overlap,correctness,"Relaxed exact match was used to measure generated answer accuracy, considering a generated answer correct if it shares a subsequence relationship with the golden answer and differs by no more than 20 characters.",https://arxiv.org/abs/2412.14457
Intersection over Union,IoU,,vision-based,attribution,"It is used to find the overlap of bounding box, calculated to determine localization precision, with an IoU threshold of 0.5 indicating a correct prediction.",https://arxiv.org/pdf/2412.14457
Mayfield-Report-Sentence-Scoring,Mayfield-Report-Sentence-Scoring,,human evaluation,"citation, correctness","Report sentence scoring. Answers to eight yes/no questions dictate an outcome for each input sentence. + indicates that the sentence is rewarded, - that it is penalized, and 0 that it does not affect the overall report score.",https://dl.acm.org/doi/10.1145/3626772.3657846
Top-k Log-Probability Drop,Top-k Log-Probability Drop,,inference-based,attribution,"Intuitively, a evidence source‚Äôs score should reflect the degree to which the generated response would change if the source were excluded.The top-klog-probability drop, measures the effect of excluding the highest-scoring sources on the probability of generating the original response",https://neurips.cc//virtual/2024/poster/96474
Linear Datamodeling Score,LDS,,inference-based,attribution,"Intuitively, a evidence source‚Äôs score should reflect the degree to which the generated response would change if the source were excluded. The linear datamodeling score (LDS), measures the extent to which attribution scores can predict the effect of excluding a random subset of sources.",https://dl.acm.org/doi/10.5555/3618408.3619536
Precision-sentence,Precision-sentence,,human evaluation,correctness,Measured how many sentences in the executive summary were factually correct and relevant compared to the human annotations.,https://dl.acm.org/doi/10.1145/3677052.3698669
Recall-sentence,Recall-sentence,,human evaluation,correctness,Assessed how many of the important sentences from the detailed summary were successfully included in the executive summary.,https://dl.acm.org/doi/10.1145/3677052.3698669
F1-sentence,F1-sentence,,human evaluation,correctness,Calculated the harmonic mean of precision and recall to provide a balanced assessment of summary quality.,https://dl.acm.org/doi/10.1145/3677052.3698669
Accuracy-Human,Accuracy-Human,,human evaluation,correctness,Accuracy for human evaluation of answer correctness.,https://neurips.cc//virtual/2024/poster/97561
Answer-Level Recall,Answer-Level Recall,,retrieval-based,attribution,Answer-Level Recall checks if the output contains any correct answers instead of exact match (identical to Hit@1),https://neurips.cc//virtual/2024/poster/95418
Usefulness-GPT-4,Usefulness-GPT-4,,llm-as-a-judge,correctness,How useful is the gernerated response to answer a question on a scale of 1-10,https://ieeexplore.ieee.org/document/10821725
Citation Recall GPT,Citation Recall GPT,,inference-based,attribution,"Adjusting Citation Recall NLI from ALCE. Ignores un-cited statements if no citation could support them, instead of penalizing all un-cited statements.",http://link.springer.com/chapter/10.1007/978-981-96-1710-4_9
Citation Precision GPT,Citation Precision GPT,,inference-based,attribution,"Adjusting Citation Precision NLI from ALCE. Accepts jointly relevant or thematically aligned citations, instead of penalizing redundant or non-essential citations.",http://link.springer.com/chapter/10.1007/978-981-96-1710-4_9
Citation F1 GPT,Citation F1 GPT,,inference-based,attribution,Adjusting Citation F1 NLI from ALCE. Harmonic mean of Citation Recall GPT and Citation Precision GPT.,http://link.springer.com/chapter/10.1007/978-981-96-1710-4_9
Character N-Gram F-Score++,CHRF++,,lexical overlap,correctness,CHRF++ computes the F-score averaged on both character and word-level n-grams to evaluate generated text against a gold/reference text.,https://aclanthology.org/W15-3049/
Hallucinated Entities,Hallucinated Entities,,human evaluation,correctness,The number of entities mentioned in the generated sentence but absent in the source graph.,https://aclanthology.org/2025.naacl-long.513/
Missed Entities,Missed Entities,,human evaluation,correctness,The number of entities present in the source graph but omitted in the generated sentence.,https://aclanthology.org/2025.naacl-long.513/
Hallucinated Relations,Hallucinated Relations,,human evaluation,correctness,The number of relations mentioned in the generated sentence but absent in the source graph.,https://aclanthology.org/2025.naacl-long.513/
Missed Relations,Missed Relations,,human evaluation,correctness,The number of relations present in the source graph but omitted in the generated sentence.,https://aclanthology.org/2025.naacl-long.513/
Grammatical Correctness and Fluency,Grammatical Correctness and Fluency,,human evaluation,linguistic,"A 5-point Likert scale score, ranging from 1-point (indicating ""very poor"") to 5-points (indicating ""highly satisfactory"").",https://aclanthology.org/2025.naacl-long.513/
Answer Accuracy GPT,Answer Accuracy GPT,,inference-based,correctness,"Calculating answer accuracy by providing the gold label reference text, question and answer, as well as the generated answer to GPT4o. Calculating the percentage of GPT4o responses that judge the generated response as correct.",https://arxiv.org/abs/2501.11929
F1 Bag-of-Words,F1 BoW,,lexical overlap,correctness,F1 BoW (macro-averaged) measures overlap between bag-of-words representation of gold and predicted answers,https://aclanthology.org/D19-1606/
Edit Accuracy Rationals,Edit Accuracy Rationals,,lexical overlap,correctness,Percentage of examples for which the revised rationale successfully incorporates feedback,https://aclanthology.org/2024.naacl-long.168/
Supported & Plausible,S&P,,human evaluation,"attribution, correctness","Asking raters to check whether either answer is a Plausible response to the question, and whether it is Supported by the accompanying quote evidence. Asking the participant to decide which answer they prefer (with ties allowed), based on these and other secondary criteria. Assessing quality of answer-evidence pair",https://arxiv.org/abs/2203.11147
Truthful & Informative,T&I,,human evaluation,correctness,"The Tuthful&Informative score assesses the quality of the answers in isolation, and was obtained via evaluation app in which the raters could see the dataset suggested correct and incorrect answers.",https://aclanthology.org/2022.acl-long.229/
Comprehensiveness-10,Comprehensiveness-10,,human evaluation,correctness,How comprehensively is the question answered? (0: lowest - 10: highest),https://arxiv.org/abs/2307.04683
Trust-10,Trust-10,,human evaluation,correctness,How trustworthy is the answer? (0: lowest - 10: highest),https://arxiv.org/abs/2307.04683
Utility-10,Utility-10,,human evaluation,relevance,How useful is the answer? (0: lowest - 10: highest),https://arxiv.org/abs/2307.04683
Citation-Relevance-10,Citation-Relevance-10,,human evaluation,citation,Relevance of top-k provided citations (0: lowest - 10: highest),https://arxiv.org/abs/2307.04683
Citation Faithfulness Rate,Citation Faithfulness,,human evaluation,citation,% of cited documents that are truly used during generation.,https://arxiv.org/abs/2412.18004
Llama-3-Eval,Llama-3-Eval,,llm-as-a-judge,user experience,"Llama-3-Eval, an open-source variant of the widely used G-Eval metric.",https://arxiv.org/abs/2412.15249
Citation Coverage,Citation Coverage,,retrieval-based,citation,Citation Coverage (in %) defines the number of citations covered in the generated response.,https://arxiv.org/abs/2412.15249
Normalized Recall@k,Normalized Recall@k,,retrieval-based,retrieval,"Unlike standard recall, which measures how many ground truth citations are retrieved, Normalized Recall@k measures how effectively the retrieval method prioritizes the most relevant papers in the top-k. By normalizing over the total relevant papers retrieved at any rank, this metric helps evaluate ranking quality independently of retrieval coverage.",https://arxiv.org/abs/2412.15249
Mean Average Recall,MAR,,retrieval-based,retrieval,MAR measures the fraction of the actual paragraphs that are covered by the predictions,https://arxiv.org/abs/2308.05361
Mean Average Precision,MAP,,retrieval-based,retrieval,MAP measures the fraction of the predictions that are correct.,https://arxiv.org/abs/2308.05361
Response-Quality-100,Response-Quality-100,,"llm-as-a-judge, human evaluation",correctness,Average of human evaluation and llm-as-a-judge scoring of response quality. (0: lowest - 100: highest),https://arxiv.org/abs/2308.05361
Citation-Accuracy-4,Citation-Accuracy-4,,human evaluation,citation,"The accuracy of each reference was assessed in terms of ‚Äòauthor(s),‚Äô ‚Äòtitle,‚Äô ‚Äòyear‚Äô and publisher/journal.‚Äô Each reference was classified as ‚Äòcorrect‚Äô, ‚Äòcorrect but incorrect year,‚Äô ‚Äòconfabulated,‚Äô and 'acknowledged as fictional.‚Äô",https://arxiv.org/abs/2308.03301
Source Attribution Accuracy@k,Source Attribution Accuracy@k,,watermark verification,attribution,"As a result, for every data provider, the accuracy of source attribution is calculated as number of correct watermarks divided by number of trials",https://arxiv.org/abs/2310.00646
Source Attribution F1,Source Attribution F1,,watermark verification,attribution,"To compute the F1 score, here we first define precision as the number of correct watermarks (watermarks that correctly correspond to its true source) for the data provider i divided by the numberof all generated watermarks that correspond to the data provider i and define recall as the number of correct watermarks divided by the number of trails of the data provider i. We calculate theprecision and recall for each data provider and obtain precision_i and recall_i. Subsequently, We obtain precision_ma and recall_ma by averaging the precisions and recalls from all data providers.",https://arxiv.org/abs/2310.00646
Distinct-N,Distinct-N,,lexical overlap,linguistic,"Distinct-N measures the diversity of generated text by calculating the ratio of unique n-grams to total n-grams, with higher values indicating greater lexical variety.",https://aclanthology.org/N16-1014/
Coherence-Tree-of-Thoughts,Coherence-ToT,,llm-as-a-judge,linguistic,Evaluates the logical flow and consistency of the text using GPT-4 in a zero-shot setting.,https://dl.acm.org/doi/abs/10.5555/3666122.3666639
Natualness-Tree-of-Thoughts,Natualness-ToT,,llm-as-a-judge,linguistic,Assesses how fluent and human-like the text is using GPT-4 in a zero-shot setting.,https://dl.acm.org/doi/abs/10.5555/3666122.3666639
CAQA F1 NLI,CAQA F1 NLI,,inference-based,attribution,"Calculating F1 score for attribution category (supportive, insufficient, contradictory and irrelevant) and overall.",https://arxiv.org/abs/2401.14640
Citation Hits@k,Citation Hits@k,,retrieval-based,citation,"Measuring the gold document ID recall over cases where the answer is correct, where recall is evaluated using Hits@k with k ‚àà {1, 10}, which measures whether the gold ID is in the top k beams",https://arxiv.org/abs/2404.01019
Correctness-3,Correctness-3,,human evaluation,correctness,"Human evaluation of correctness (correct, incorrect or refusal). A response is correct if it is both factually correct and relevant to the query. A response is incorrect if it contains any factually inaccurate information. If a response is neither correct nor incorrect, because the model simply declines to respond, we label that as a refusal.","https://arxiv.org/abs/2405.20362, https://arxiv.org/abs/2407.03004"
Groundedness-3,Groundedness-3,,human evaluation,citation,"Human evaluation of groundedness (grounded, misgrounded or ungrounded). A response is grounded if the key factual propositions in its response make valid references to relevant legal documents. A response is ungrounded if key factual propositions are not cited. A response is misgrounded if key factual propositions are cited but misinterpret the source or reference an inapplicable source.",https://arxiv.org/abs/2405.20362
Hallucination-2,Hallucination-2,,human evaluation,correctness,Human evaluation of hallucination (yes or no). A response is considered hallucinated if it is either incorrect or misgrounded.,https://arxiv.org/abs/2405.20362
Reponse-Accuracy-2,Reponse-Accuracy-2,,human evaluation,correctness,Human evaluation of reponse accuracy (yes or no). Accurate responses are those that are both correct and grounded.,https://arxiv.org/abs/2405.20362
Response-Incompleteness-2,Response-Incompleteness-2,,human evaluation,correctness,Human evaluation of reponse incompleteness (yes or no). Incomplete responses are those that are either refusals or ungrounded.,https://arxiv.org/abs/2405.20362
Token F1,Token F1,,lexical overlap,correctness,Token-level F1 compares the overlap between the predicted and gold answer tokens. It is the harmonic mean of token precision (fraction of predicted tokens that are correct) and token recall (fraction of gold tokens that were predicted).,https://arxiv.org/abs/2406.13632
Supporting Paragraph F1,Supporting Paragraph F1,,retrieval-based,attribution,This metric evaluates how well the model identified the correct supporting paragraphs from the context. It computes the F1 score between predicted and gold evidence paragraph sets.,https://arxiv.org/abs/2406.13632
Positional Fine-grained Citation Recall,PFCR,ALiiCE,inference-based,attribution,"This metric assesses whether each atomic claim within a sentence is supported by the cited sources. An atomic claim is a minimal, indivisible assertion extracted via dependency parsing. For each atomic claim, if the concatenated cited passages entail the claim (as determined by a Natural Language Inference model), the recall is 1; otherwise, it's 0. The overall recall is the average across all atomic claims.",https://arxiv.org/abs/2406.13375
Positional Fine-grained Citation Precision,PFCP,ALiiCE,inference-based,attribution,"This metric evaluates the necessity of each citation. For atomic claims with a recall of 1, it checks each cited passage to determine if it's essential. If a cited passage alone does not entail the claim, but the combined citations do, it's considered redundant, resulting in a precision of 0 for that citation. The overall precision is the mean across all such evaluations. ",https://arxiv.org/abs/2406.13375
Positional Fine-grained Citation F1,PFCF1,ALiiCE,inference-based,attribution,Harmonic mean of PFCR and PFCP.,https://arxiv.org/abs/2406.13375
Coefficient of Variation of Citation Positions,CVCP,ALiiCE,structure-based,citation,"CVCP measures the dispersion of citation markers within a sentence. It's calculated by normalizing the positions of citation markers relative to sentence length, computing their standard deviation, and then dividing by the mean. A higher CVCP indicates that citations are more evenly distributed throughout the sentence, suggesting better fine-grained citation placement.",https://arxiv.org/abs/2406.13375
Set Precision,Set Precision,,retrieval-based,attribution,Measures the proportion of correctly identified citations among all citations predicted by the annotator (either human or LLM) based on the masked explanation.,https://arxiv.org/abs/2406.12645
Set Recall,Set Recall,,retrieval-based,attribution,Measures the proportion of ground-truth citations that were correctly recovered by the annotator from the masked explanation.,https://arxiv.org/abs/2406.12645
Set F1,Set F1,,retrieval-based,attribution,The harmonic mean of set precision and set recall.,https://arxiv.org/abs/2406.12645
Completeness-3,Completeness-3,,human evaluation,correctness,"Assesses whether the model's reasoning fully addresses all relevant aspects required for medical interpretation (complete, somewhat complete, incomplete).",https://arxiv.org/abs/2407.03004
Citation Accuracy-2,Citation Accuracy-2,,human evaluation,citation,"Determines if the sources cited by the model are real and correctly matched, considering a citation correct only if the author list and exact title match verbatim (yes or no).",https://arxiv.org/abs/2407.03004
Reading Comprehension-2,Reading Comprehension-2,,human evaluation,linguistic,Evaluates the model's ability to correctly interpret and restate the clinical narrative (yes or no).,https://arxiv.org/abs/2407.03004
Recall Of Knowledge-2,Recall Of Knowledge-2,,human evaluation,correctness,"Assesses whether the model brings in relevant domain knowledge beyond the narrative, such as typical semiology patterns (yes or no).",https://arxiv.org/abs/2407.03004
Reasoning Step-2,Reasoning Step-2,,human evaluation,correctness,Evaluates how well the model connects the narrative and knowledge to reach a clinical inference (yes o no).,https://arxiv.org/abs/2407.03004
Correct Attribution Score,CAS,,inference-based,attribution,"Correct attribution score, the proportion of sentences predicted as correctly attributed among all the sentences in the answer.",https://arxiv.org/abs/2407.01796
Citation Redundancy Score,CRS,,lexical overlap,attribution,"Citation redundancy score, the proportion of non-redundant sentences relative to all sentences in the references.",https://arxiv.org/abs/2407.01796
Consistency Ratio,CR,,lexical overlap,attribution,"Consistency ratio, the text consistency between the reference parts and the reference passages through string matching.",https://arxiv.org/abs/2407.01796
Attribution Ratio,AR,,lexical overlap,attribution,"Attribution ratio, the proportion of sentences in the output that are attributed.",https://arxiv.org/abs/2407.01796
Appropriate Citation Rate,Appropriate Citation Rate,CiteKit,inference-based,attribution,This metric measures the proportion of statements in a generated response that are correctly supported by appropriate citations.,https://arxiv.org/abs/2408.04662
Citation Granularity,Citation Granularity,CiteKit,retrieval-based,citation,"This metric assesses how specific the citations are, evaluating whether they refer to entire documents, summaries, or precise snippets.",https://arxiv.org/abs/2408.04662
Verification Accuracy,Verification Accuracy,,inference-based,correctness,"Classification Accuracy (supported, refuted, not enough information (NEI))",https://arxiv.org/abs/2407.19813
RAGAS Faithfulness ,RAGAS Faithfulness ,RAGAS,inference-based,attribution,"Evaluates how well the generated answer is supported by the retrieved context, by verifying if each claim in the answer can be logically inferred from the context, using an LLM-based entailment check.",https://aclanthology.org/2024.eacl-demo.16/
RAGAS Answer-Relevancy,RAGAS Answer-Relevancy,RAGAS,semantic similarity-based,relevance,Measures the relevance of the generated answer to the original question by computing the average semantic similarity between the original question and a set of questions generated from the answer.,https://aclanthology.org/2024.eacl-demo.16/
RAGAS Context-Precision,RAGAS Context-Precision,RAGAS,retrieval-based,relevance,"Measures how much of the retrieved context is relevant by computing the proportion of context that aligns with the ground-truth answer, emphasizing concise and focused retrieval.",https://aclanthology.org/2024.eacl-demo.16/
RAGAS Context-Recall,RAGAS Context-Recall,RAGAS,retrieval-based,relevance,"Measures the extent to which the retrieved context covers information from the gold answer, assessing whether key facts are included in retrieval.",https://aclanthology.org/2024.eacl-demo.16/
RAGAS Context-Relevancy,RAGAS Context-Relevancy,RAGAS,llm-as-a-judge,relevance,"Uses an LLM to extract relevant sentences from the context that help answer the question, scoring the proportion of useful information relative to the full context.",https://aclanthology.org/2024.eacl-demo.16/
RAGAS Context-Entity-Recall,RAGAS Context-Entity-Recall,RAGAS,retrieval-based,retrieval,"Calculates the fraction of entities in the ground-truth answer that appear in the retrieved context, assessing entity-level recall in the retrieval process.",https://aclanthology.org/2024.eacl-demo.16/
RAGAS Answer-Semantic-Similarity,RAGAS Answer-Semantic-Similarity,RAGAS,semantic similarity-based,correctness,"Computes the semantic similarity between the generated answer and the reference answer using vector embeddings, capturing paraphrased or synonymous content overlap.",https://aclanthology.org/2024.eacl-demo.16/
RAGAS Answer-Correctness,RAGAS Answer-Correctness,RAGAS,inference-based,correctness,"Evaluates the factual accuracy of the generated answer by comparing it to the gold answer, using both claim-level matching and entailment-based verification.",https://aclanthology.org/2024.eacl-demo.16/
Link Prediction Accuracy,Link Prediction Accuracy,,retrieval-based,citation,Link prediction accuracy measures the proportion of correctly predicted citation links between documents compared to the actual links in a gold-standard citation graph.,https://arxiv.org/abs/2409.12177
Correctness Ratio,CR,,llm-as-a-judge,correctness,"Correctness Ratio (CR) measures the factual accuracy of model-generated answers by using GPT-4o to rate them based on the query and ground-truth answers, with citation markers removed (Rating 1-10). It quantifies the proportion of responses judged as correct, comparing citation-enhanced answers to standard ones.",https://arxiv.org/abs/2409.02897
Factuality-of-Evidence-3,Factuality-of-Evidence-3,,human evaluation,citation,"Human evaluation of top 3 references: 1) correct references, where the references are real with correct metadata, (2) references with minor errors, where the references are real but have minor metadata errors, or (3) hallucinated references, where the references do not exist.",https://arxiv.org/abs/2409.13902
Relevance-of-Evidence-Retrieval,Relevance-of-Evidence-Retrieval,,retrieval-based,retrieval,Relevance-of-Evidence-Retrieval measures how well the LLM selects from the most relevant documents retrieved by RAG. It calculates the proportion of top-10 retrieved documents that appear among the LLM's top 3 references and the average rank of those selected documents. This evaluates alignment between retrieval quality and the LLM‚Äôs citation choices.,https://arxiv.org/abs/2409.13902
Reponse-Accuracy-5,Reponse-Accuracy-5,,human evaluation,correctness,Human evaluation of Response-Accuracy from 1 (poor) to 5 (perfefct),https://arxiv.org/abs/2409.13902
Completeness-5,Completeness-5,,human evaluation,correctness,Human evaluation of Completness from 1 (poor) to 5 (perfefct),https://arxiv.org/abs/2409.13902
Evidence-Attribution-5,Evidence-Attribution-5,,human evaluation,attribution,Human evaluation of Evidence-Attribution from 1 (poor) to 5 (perfefct),https://arxiv.org/abs/2409.13902
AEE-One-Sided Answer,AEE-One-Sided Answer,AEE,llm-as-a-judge,correctness,Measures whether a response presents only one side of a contentious or multi-faceted issue without acknowledging alternative perspectives.,https://arxiv.org/abs/2401.08590
AEE-Overconfident Answer,AEE-Overconfident Answer,AEE,llm-as-a-judge,correctness,Evaluates if a response displays unjustified certainty especially on topics with limited evidence or ongoing debates.,https://arxiv.org/abs/2401.08590
AEE-Relevant Statements,AEE-Relevant Statements,AEE,llm-as-a-judge,relevance,Measures the proportion of statements in a response that are directly relevant to answering the query.,https://arxiv.org/abs/2401.08590
AEE-Uncited Sources,AEE-Uncited Sources,AEE,retrieval-based,citation,Identifies claims or statements that should have citations but are presented without attribution to any source.,https://arxiv.org/abs/2401.08590
AEE-Source Necessity,AEE-Source Necessity,AEE,llm-as-a-judge,citation,Evaluates whether the types of sources cited are appropriate and necessary for the claims being made.,https://arxiv.org/abs/2401.08590
AEE-Unsupported Statements,AEE-Unsupported Statements,AEE,llm-as-a-judge,attribution,Identifies factual claims made in a response that lack supporting evidence or cannot be verified by the provided citations.,https://arxiv.org/abs/2401.08590
AEE-Citation Thoroughness,AEE-Citation Thoroughness,AEE,llm-as-a-judge,citation,Evaluates the comprehensiveness of citations across all factual claims that warrant attribution.,https://arxiv.org/abs/2401.08590
AEE-Citation Accuracy,AEE-Citation Accuracy,AEE,llm-as-a-judge,citation,Measures whether the cited sources actually support the claims made in the response.,https://arxiv.org/abs/2401.08590
Attribution Recall,Attr_r,,inference-based,attribution,"This metric measures the fraction of atomic facts in the generated answer that are entailed by any retrieved evidence, indicating how many claims are correctly attributed. It is closely related to Auto-AIS-sentence.",https://arxiv.org/abs/2410.16708
Attribution Precision,Attr_p,,inference-based,attribution,"This metric measures the fraction of retrieved evidence snippets that are entailed by at least one atomic fact in the answer, reflecting how much of the retrieved content is actually relevant. It is closely related to Auto-AIS-sentence",https://arxiv.org/abs/2410.16708
F1 Recall-Preservation,F1_rp,,"inference-based, lexical overlap","attribution, preservation",Harmonic mean of Attr_r and Pres-Lev,https://arxiv.org/abs/2410.16708
F1 Precision-Preservation,F1_pp,,"inference-based, lexical overlap","attribution, preservation",Harmonic mean of Attr_p and Pres-Lev,https://arxiv.org/abs/2410.16708
BioGen-Answer Accuracy,BioGen-Answer Accuracy,BioGen,human evaluation,correctness,Measures how many of the answers to the total of 65 questions were deemed acceptable (judged as answering the question at least partially) for each run.,
BioGen-Answer Completeness,BioGen-Answer Completeness,BioGen,semantic similarity-based,correctness,Measures how many of the answer aspects are covered in one answer.,https://arxiv.org/pdf/2411.18069
Biogen-Answer Precision,Biogen-Answer Precision,BioGen,human evaluation,correctness,Measures how many of the assertions in the answer were judged required or acceptable.,https://arxiv.org/pdf/2411.18069
BioGen-Redundancy Score,BioGen-Redundancy Score,BioGen,human evaluation,linguistic,Penalizes a system for generating unnecessary answer sentences; measures informativeness.,https://arxiv.org/pdf/2411.18069
BioGen-Irrelevant Score,BioGen-Irrelevant Score,BioGen,human evaluation,relevance,Penalizes a system for generating inappropriate potentially harmful answer sentences.,https://arxiv.org/pdf/2411.18069
BioGen-Citation Coverage,BioGen-Citation Coverage,BioGen,human evaluation,citation,Measures how well the required and borderline generated answer sentences are backed by the appropriate (judged as supports) citations.,https://arxiv.org/pdf/2411.18069
BioGen-Citation Support Rate,BioGen-Citation Support Rate,BioGen,human evaluation,citation,Assesses how well the system-predicted citations are aligned with the human-judged support citations.,https://arxiv.org/pdf/2411.18069
BioGen-Citation Contradict Rate,BioGen-Citation Contradict Rate,BioGen,human evaluation,citation,Penalizes the answers that are providing documents assessed as Contradicting the statement.,https://arxiv.org/pdf/2411.18069
BioGen-Document Relevancy Recall,BioGen-Document Relevancy Recall,BioGen,retrieval-based,retrieval,Measures the proportion of relevant retrieved documents out of all relevant documents.,https://arxiv.org/pdf/2411.18069
BioGen-Document Relevancy Precision,BioGen-Document Relevancy Precision,BioGen,retrieval-based,retrieval,Measures the proportion of relevant retrieved documents out of the number of provided references.,https://arxiv.org/pdf/2411.18069
Fluency-3,Fluency-3,,human evaluation,linguistic,"Annotators count misprints, incoherent sentences, and abrupt transitions.",https://arxiv.org/pdf/2411.17375
Perceived Utility-3,Perceived Utility-3,,human evaluation,relevance,"Annotators assess how well a generation addresses the query, its conciseness, and style appropriateness.",https://arxiv.org/pdf/2411.17375
Time to Verify,T2V,,human evaluation,citation,Measures the time taken to verify if a generated sentence is covered by its citations.,https://arxiv.org/pdf/2411.17375
Citation Precision-2,Citation Precision-2,,human evaluation,citation,"Evaluates whether each citation supports at least one claim in the sentence it appears in, using binary (yes/no) labels.",https://arxiv.org/abs/2411.17375
Citation Coverage-2,Citation Coverage-2,,human evaluation,citation,"Determines whether all claims in a sentence are supported by the accompanying citations, labeled as yes or no.",https://arxiv.org/abs/2411.17375
ScholarQABench-Correctness,ScholarQABench-Corr,ScholarQABench,llm-as-a-judge,correctness,Evaluates the degree of overlap or matching of a model-generated answer and human-annotated reference.,https://arxiv.org/pdf/2411.14201
ScholarQABench-Relevance,ScholarQABench-Rel,ScholarQABench,llm-as-a-judge,relevance,Assesses if the generated answers are relevant to the question.,https://arxiv.org/pdf/2411.14201
ScholarQABench-Organization and Writing Flow,ScholarQABench-Org,ScholarQABench,llm-as-a-judge,linguistic,Assesses the organization and writing flow of the generated answers.,https://arxiv.org/pdf/2411.14201
ScholarQABench-Overall Usefulness,ScholarQABench-Use,ScholarQABench,llm-as-a-judge,relevance,Evaluates the overall utility and helpfulness of the generated response.,https://arxiv.org/pdf/2411.14201
Attack Success Rate,ASR,,llm-as-a-judge,correctness,Measures the rate at which the jailbreak attack successfully elicits harmful content from the LLM.,https://arxiv.org/pdf/2411.11407
Defense Pass Rate,DPR,,llm-as-a-judge,correctness,Measures the rate at which the defense mechanism prevents the generation of harmful content.,https://arxiv.org/pdf/2411.11407
Kullback-Leibler Divergence,KL Divergence,,inference-based,linguistic,Measures the difference in probability distributions between generated text with and without defense mechanisms.,https://arxiv.org/pdf/2411.11407
Counterfactually-estimated Attribution Sensitivity Recall,CAS Recall,,retrieval-based,citation,"Measures the change in recall of correctly attributed documents when authorship metadata is added to the input documents. It quantifies how sensitive the model's recall performance is to the presence of authorship labels (e.g., [Human] vs. [LLM]).",https://arxiv.org/abs/2410.12380
Counterfactually-estimated Attribution Sensitivity Precision,CAS Precision,,retrieval-based,citation,"Captures the change in precision of attributions when authorship metadata is included. It assesses whether the inclusion of metadata alters the accuracy of cited documents, indicating model sensitivity to this metadata in its citation behavior.",https://arxiv.org/abs/2410.12380
Counterfactually-estimated Attribution Bias Recall,CAB Recall,,retrieval-based,citation,"Quantifies the difference in attribution recall when documents are labeled as written by humans vs. LLMs, holding content constant. It evaluates the bias in recalling documents depending on their authorship label.",https://arxiv.org/abs/2410.12380
Counterfactually-estimated Attribution Bias Precision,CAB Precision,,retrieval-based,citation,"Measures the difference in precision of attributions when the same documents are attributed to humans vs. LLMs. It reflects how much more precisely a model cites documents based on perceived authorship, indicating bias.",https://arxiv.org/abs/2410.12380
Attribution Confidence,AC,,inference-based,citation,"Defined as the average model-assigned probability of generating attribution tokens (e.g., citation numbers) in the answer. It indicates the confidence level of the model when making attributions, often correlating with bias toward human-labeled content.",https://arxiv.org/abs/2410.12380
Normalized Discounted Cumulative Gain@k,nDCG@k,QUILL,retrieval-based,retrieval,Normalized discounted cumulative gain at rank k; rewards putting the correct quote higher in the ranking.,https://arxiv.org/abs/2411.03675
Quotation Authenticity,Quotation Authenticity,QUILL,llm-as-a-judge,citation,"Checks whether the string the model produced can be verified as an authentic, externally documented quotation, thereby detecting hallucinated/fabricated quotes.",https://arxiv.org/abs/2411.03675
Quotation Credibility,Quotation Credibility,QUILL,llm-as-a-judge,citation,"Given any author/source already mentioned in the surrounding text, the metric verifies with GPT-4o and search evidence, whether the generated quote is actually from that author/source; returns 1 / 0.",https://arxiv.org/abs/2411.03675
Semantic Matching,Semantic Matching,QUILL,llm-as-a-judge,relevance,Computes perplexity of the right context cr conditioned on the left context + quote; lower perlexity ‚áí quote semantically fits the paragraph. The perplexity is rescaled to 0-1 with an inverted sigmoid.,https://arxiv.org/abs/2411.03675
Semantic Fluency,Semantic Fluency,QUILL,llm-as-a-judge,linguistic,"Runs perplexity over the entire paragraph after inserting the quotation; lower perplexity ‚áí smoother, more coherent prose. Also mapped to 0-1 with a sigmoid.",https://arxiv.org/abs/2411.03675
Quotation Novelty,Quotation Novelty,QUILL,llm-as-a-judge,novelty,"Scores Novelty=perlexity(quote) / log10‚Äã(Bing hits))‚Äã; uncommon quotes with high perplexity and few search results get higher scores, encouraging original or less-clich√© quotations.",https://arxiv.org/abs/2411.03675
Accuracy-2,Accuracy-2,,human evaluation,correctness,Evaluators judged whether the information in the model's response was factually correct or not. A response was marked as accurate or inaccurate based on medical correctness.,https://arxiv.org/abs/2411.02657
Thouroughness-2,Thouroughness-2,,human evaluation,correctness,"Evaluators assessed whether the response fully addressed the user's question. It was labeled as thorough if it covered all relevant aspects, or not thorough if important elements were missing.",https://arxiv.org/abs/2411.02657
Clarity-2,Clarity-2,,human evaluation,linguistic,"This metric measured how understandable and well-articulated the response was. Responses were marked as clear if they were coherent and easy to comprehend, or unclear otherwise.",https://arxiv.org/abs/2411.02657
Per-Response Citation Accuracy-2,Per-Response Citation Accuracy-2,,human evaluation,citation,Each citation provided in a response was judged as either correct (accurately supporting the associated claim) or incorrect. This measured the factual reliability of individual citations.,https://arxiv.org/abs/2411.02657
Percentage of Responses with All Correct Citations-2,Percentage of Responses with All Correct Citations-2,,human evaluation,citation,"Responses were labeled as correctly cited if all citations in the response were accurate; otherwise, the response was considered to have incomplete or incorrect citations.",https://arxiv.org/abs/2411.02657
Faithfulness-REC,Faithfulness-REC,REC,llm-as-a-judge,correctness,"Assesses whether the generated content accurately reflects the source information, ensuring factual correctness.",https://arxiv.org/abs/2411.02448
Instruction Following-REC,Instruction Following-REC,REC,llm-as-a-judge,relevance,Evaluates how well the generated response adheres to the given instructions or prompts.,https://arxiv.org/abs/2411.02448
Coherence-REC,Coherence-REC,REC,llm-as-a-judge,linguistic,"Measures the logical flow and consistency within the generated text, ensuring it is well-structured and understandable.",https://arxiv.org/abs/2411.02448
Completeness-REC,Completeness-REC,REC,llm-as-a-judge,correctness,"Determines if the generated response fully addresses all aspects of the prompt or question, covering necessary details.",https://arxiv.org/abs/2411.02448
Citation-REC,Citation-REC,REC,llm-as-a-judge,citation,"Evaluates the presence and quality of citations in the generated text, checking if claims are supported by appropriate sources.",https://arxiv.org/abs/2411.02448
BLEU-N,BLEU-N,,lexical overlap,"correctness, attribution",BLEU-1 is a metric that measures the overlap of individual words (unigrams) between a generated text and a reference text. It is commonly used to evaluate how closely a machine-generated response matches a human-written one based on word-level similarity.,https://dl.acm.org/doi/10.3115/1073083.1073135
Roy-Answer Relevance,Roy-Answer Relevance,,llm-as-a-judge,correctness,"GPT-4o scores the relevance of generated answers by comparing them to gold reference answers on a relevance scale (0 for non-relevant, 0.5 for partially relevant, 1 for relevant).",https://arxiv.org/abs/2412.10571
Roy-Attribution Accuracy,Roy-Attribution Accuracy,,inference-based,attribution,Measures whether the evidence used by the model to generate an answer can be causally attributed to the retrieved documents. The focus is not on citation format but on whether the retrieved documents contributed to the answer generation.,https://arxiv.org/abs/2412.10571
Helpful-General,Helpful-General,RAG-RewardBench,llm-as-a-judge,relevance,Assesses the overall helpfulness of the response in addressing the user's query.,https://arxiv.org/abs/2412.13746
Helpful-Reason,Helpful-Reason,RAG-RewardBench,llm-as-a-judge,correctness,Evaluates the logical coherence and reasoning quality within the response.,https://arxiv.org/abs/2412.13746
Helpful-Citation,Helpful-Citation,RAG-RewardBench,llm-as-a-judge,citation,Measures the accuracy and appropriateness of citations provided to support the response.,https://arxiv.org/abs/2412.13746
Helpful-Average,Helpful-Average,RAG-RewardBench,llm-as-a-judge,"relevance, correctness, citation",Average score across the above helpfulness metrics.,https://arxiv.org/abs/2412.13746
Harmless-General,Harmless-General,RAG-RewardBench,llm-as-a-judge,user experience,"Assesses the overall harmlessness of the response, ensuring it does not contain harmful or inappropriate content.",https://arxiv.org/abs/2412.13746
Harmless-Abstain,Harmless-Abstain,RAG-RewardBench,llm-as-a-judge,correctness,"Evaluates the model's ability to abstain from answering when appropriate, particularly when the information is insufficient or uncertain.",https://arxiv.org/abs/2412.13746
Harmless-Conflict,Harmless-Conflict,RAG-RewardBench,llm-as-a-judge,correctness,"Measures the model's robustness in handling conflicting information, ensuring it does not propagate contradictions.",https://arxiv.org/abs/2412.13746
Harmless-Average,Harmless-Average,RAG-RewardBench,llm-as-a-judge,"user experience, correctness",Average score across the above harmlessness metrics.,https://arxiv.org/abs/2412.13746
RAG-RewardBench-Overall,RAG-RewardBench-Overall,RAG-RewardBench,llm-as-a-judge,"relevance, correctness, citation, user experience",Final aggregate score reflecting all other metrics.,https://arxiv.org/abs/2412.13746
Average Normalized Longest Common Subsequence,ANLCS,VisDoMBench,lexical overlap,retrieval,Evidence extraction is assessed using ANLCS between ground truth evidence and retrieved chunks/pages.,https://arxiv.org/abs/2412.10704
Document Identification Rate,Document Identification Rate,VisDoMBench,retrieval-based,retrieval,Computes the proportion of instances in which the correct source document (containing ground-truth evidence) is the majority contributor to the retrieved context. This tests whether retrieval selects relevant documents in a multi-document setup.,https://arxiv.org/abs/2412.10704
Word Overlap F1,Word Overlap F1,VisDoMBench,lexical overlap,correctness,"For PaperTab,the modified implementation of Word Overlap F1 from (Hui et al., 2024), which takes into account different answer types (binary, short text) is used. For all other datasets, the Word Overlap F1, which serves as a flexible metric to evaluate different answer types is used.",https://arxiv.org/abs/2412.10704
Token-Level Recall,Token-Level Recall,,retrieval-based,attribution,Measures the proportion of source tokens correctly identified by the model out of all tokens that should have been attributed.,https://aclanthology.org/2024.findings-acl.682/
Token-Level Precision,Token-Level Precision,,retrieval-based,attribution,Measures the proportion of tokens the model attributed to a source that are actually correct according to ground truth.,https://aclanthology.org/2024.findings-acl.682/
Token-Level F1,Token-Level F1,,retrieval-based,attribution,"The harmonic mean of token-level precision and recall, balancing accuracy and completeness in attribution.",https://aclanthology.org/2024.findings-acl.682/
Span-Level Accuracy,Span-Level Accuracy,,retrieval-based,attribution,"Measures the percentage of predicted spans that exactly match the ground truth spans, assessing correctness at the span level.",https://aclanthology.org/2024.findings-acl.682/